{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, install and import required libraries, watermark our file, initialise our Spark Session Builder and initialise our environment with required configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watermark in /opt/app-root/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: ipython in /opt/app-root/lib/python3.8/site-packages (from watermark) (7.22.0)\n",
      "Requirement already satisfied: backcall in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (0.17.2)\n",
      "Requirement already satisfied: decorator in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (5.0.7)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (57.4.0)\n",
      "Requirement already satisfied: pickleshare in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (5.0.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (3.0.18)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (2.8.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from jedi>=0.16->ipython->watermark) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/app-root/lib/python3.8/site-packages (from pexpect>4.3->ipython->watermark) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /opt/app-root/lib/python3.8/site-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Minio in /opt/app-root/lib/python3.8/site-packages (6.0.2)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.8/site-packages (from Minio) (2020.12.5)\n",
      "Requirement already satisfied: pytz in /opt/app-root/lib/python3.8/site-packages (from Minio) (2021.1)\n",
      "Requirement already satisfied: urllib3 in /opt/app-root/lib/python3.8/site-packages (from Minio) (1.26.4)\n",
      "Requirement already satisfied: configparser in /opt/app-root/lib/python3.8/site-packages (from Minio) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/app-root/lib/python3.8/site-packages (from Minio) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.8/site-packages (from python-dateutil->Minio) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/app-root/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (4.30.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install watermark\n",
    "%pip install Minio\n",
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "import watermark\n",
    "from minio import Minio\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.6\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : GCC 8.4.1 20200928 (Red Hat 8.4.1-1)\n",
      "OS          : Linux\n",
      "Release     : 4.18.0-305.34.2.el8_4.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 9f7107aa0927f093c75a4e37017c6b0c38425c71\n",
      "\n",
      "watermark: 2.3.0\n",
      "json     : 2.0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDENT CONFIGURATION\n",
      "=====================\n",
      "User ID: \"user29\"\n",
      "Project name: \"CustomerChurn-user29\"\n",
      "Experiment name: \"CustomerChurn-user29\", \"customerchurn-user29\"\n",
      "S3 Bucket full storage path: \"full_data_csv-user29/your.csv\"\n"
     ]
    }
   ],
   "source": [
    "from config import get_params\n",
    "user_id,PROJECT_NAME,EXPERIMENT_NAME,experiment_name, s3BucketFullPath = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkSessionBuilder = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Customer Churn ingest Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# submit_args = \"--conf spark.jars.ivy=/tmp \\\n",
    "# --conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "# --conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "# --conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "# --conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "# --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "# --packages org.apache.hadoop:hadoop-aws:3.2.0\"\n",
    "\n",
    "submit_args = \"--conf spark.jars.ivy=/tmp \\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--packages org.apache.hadoop:hadoop-aws:3.2.0,\\\n",
    "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,\\\n",
    "org.apache.kafka:kafka-clients:2.8.0,\\\n",
    "org.apache.spark:spark-streaming_2.12:3.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment variables for Spark\n",
      "Creating a spark session...\n",
      "Spark session created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <br/>\n",
       "            <p><b>Spark Context</b></p>\n",
       "            <dl>\n",
       "              <dt>Cluster Name</dt>\n",
       "                <dd><code>spark-cluster-user29</code></dd>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-cluster-user29:7077</code></dd>\n",
       "              <dt>App Id</dt>\n",
       "                <dd><code>app-20220311045421-0002</code></dd>\n",
       "              <dt>App Name</dt>\n",
       "                <dd><code>ML Ops Demo</code></dd>\n",
       "              <dt>Driver IP</dt>\n",
       "                <dd><code>10.129.2.26</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context started.\n"
     ]
    }
   ],
   "source": [
    "import spark_util\n",
    "\n",
    "spark = spark_util.getOrCreateSparkSession(\"ML Ops Demo\", submit_args)\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print('Spark context started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Declare our input data sources, import and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame_Customer = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/customers/Customer-Churn_P1.csv\")\n",
    "dataFrame_Customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataFrame_Products = spark.read\\\n",
    "#                 .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "#                 .csv(\"s3a://rawdata/products/Customer-Churn_P2.csv\")\n",
    "# dataFrame_Products.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: string (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "+----------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+----------------+--------------+------------+-----+\n",
      "|customerID|PhoneService|   MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|   PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+----------------+--------------+------------+-----+\n",
      "|         1|          No|No phone service|            DSL|            No|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|Electronic check|         29.85|       29.85|   No|\n",
      "|         2|         Yes|              No|            DSL|           Yes|          No|             Yes|         No|         No|             No|      One year|              No|    Mailed check|         56.95|      1889.5|   No|\n",
      "+----------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+----------------+--------------+------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from  pyspark.sql.functions import *\n",
    "\n",
    "srcKafkaBrokers = \"odh-message-bus-kafka-bootstrap:9092\"\n",
    "srcKakaTopic = \"datatelco\"\n",
    "\n",
    "\n",
    "\n",
    "schema = StructType()\\\n",
    "    .add(\"customerID\", IntegerType())\\\n",
    "    .add(\"PhoneService\", StringType())\\\n",
    "    .add(\"MultipleLines\", StringType())\\\n",
    "    .add(\"InternetService\", StringType())\\\n",
    "    .add(\"OnlineSecurity\", StringType())\\\n",
    "    .add(\"OnlineBackup\", StringType())\\\n",
    "    .add(\"DeviceProtection\", StringType())\\\n",
    "    .add(\"TechSupport\", StringType())\\\n",
    "    .add(\"StreamingTV\", StringType())\\\n",
    "    .add(\"StreamingMovies\", StringType())\\\n",
    "    .add(\"Contract\", StringType())\\\n",
    "    .add(\"PaperlessBilling\", StringType())\\\n",
    "    .add(\"PaymentMethod\", StringType())\\\n",
    "    .add(\"MonthlyCharges\", StringType())\\\n",
    "    .add(\"TotalCharges\", DoubleType())\\\n",
    "    .add(\"Churn\", StringType())\n",
    "\n",
    "\n",
    "\n",
    "#Read from JSON Kafka messages into a dataframe\n",
    "dataFrame_Products = spark.read.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", srcKafkaBrokers)\\\n",
    "    .option(\"subscribe\", srcKakaTopic)\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\\\n",
    "    .withColumn(\"value\", regexp_replace(col(\"value\").cast(\"string\"), \"\\\\\\\\\", \"\")) \\\n",
    "    .withColumn(\"value\", regexp_replace(col(\"value\"), \"^\\\"|\\\"$\", \"\")) \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as jsonValue\")\\\n",
    "    .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "dfObj = spark.read.schema(schema).json(dataFrame_Products)\n",
    "dfObj.printSchema()\n",
    "dfObj.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrom_All = dataFrame_Customer.join(dfObj, \"customerID\", how=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push prepared data to object storage and stop Spark cluster to save resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"s3a://data/full_data_csv-\" + user_id\n",
    "dataFrom_All.repartition(1).write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook complete!\n"
     ]
    }
   ],
   "source": [
    "print('Notebook complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
