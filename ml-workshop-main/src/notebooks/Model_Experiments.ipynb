{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage.\n",
    "\n",
    "### First, install and import required libraries and watermark our file - to show what libraries and versions we're using. Then define utility functions to integrate with our Object storage and _Verta_ visualisation server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %horus requirements --add matplotlib\n",
    "\n",
    "%pip install matplotlib\n",
    "%pip install pandas==1.1.5\n",
    "%pip install scikit-learn\n",
    "%pip install minio\n",
    "%pip install seaborn\n",
    "%pip install mlflow\n",
    "%pip install ipynbname\n",
    "%pip install category_encoders==2.2.2\n",
    "%pip install joblib\n",
    "%pip install watermark\n",
    "%pip install boto3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "\n",
    "import watermark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# import tools as tools\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Lab parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from config import get_params, download_csv_files\n",
    "user_id,PROJECT_NAME,EXPERIMENT_NAME,experiment_name, s3BucketFullPath = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### In this next section, we initialise our variables and our Object Storage implementation, Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HOST = \"http://mlflow:5500\"\n",
    "\n",
    "#PROJECT_NAME = \"CustomerChurnUser60\"\n",
    "#EXPERIMENT_NAME = \"CustomerChurnUser60\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio-ml-workshop:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='minio123'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='mlflow'\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%Y%H%M%S%f\")\n",
    "experiment_id = experiment_name + timestampStr\n",
    "\n",
    "\n",
    "def get_s3_server():\n",
    "    minioClient = Minio('minio-ml-workshop:9000',\n",
    "                    access_key=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                    secret_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Connect to local MLflow tracking server\n",
    "mlflow.set_tracking_uri(HOST)\n",
    "\n",
    "# Set the experiment name...\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "mlflow.sklearn.autolog(log_input_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### In this next section, we pull in the merged CSV file prepared earlier by the data engineer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minioClient = get_s3_server()\n",
    "# data_file = minioClient.fget_object(\"data\", s3BucketFullPath, \"/tmp/data.csv\")\n",
    "# data_file_version = data_file.version_id\n",
    "download_csv_files(minioClient, s3BucketFullPath)\n",
    "data = pd.read_csv('/tmp/data.csv')\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Use pandas.DataFrame functions\n",
    "### - _shape_ to return the dimensionality\n",
    "### - _info_ to print a concise summary of the DataFrame\n",
    "### - _describe_ to generate descriptive statistics of the DataFrame's columns\n",
    "### - _isnull().sum()_ to sum the empty values\n",
    "### - finally determine Churn and Total Changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert binary variable into numeric so plotting is easier. We need to later take mean\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.replace(\" \", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean = data['TotalCharges'].mean()\n",
    "data.fillna(mean, inplace=True)\n",
    "# Now we know that total charges has nan values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature Engineering pipeline\n",
    "### Use category_encoder's Ordinal encoding method which uses a single column of integers to represent the classes - then fit that to our 2 dimensional data imported earlier. Then pickle it and transform it. Then use Onehot (or dummy) coding for categorical features, producing one feature per category, each binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import joblib\n",
    "\n",
    "names = ['gender', 'Partner', 'Dependents', 'PhoneService', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Churn']\n",
    "# for column in names:\n",
    "#     labelencoder(column)\n",
    "\n",
    "enc = ce.OrdinalEncoder(cols=names)\n",
    "enc.fit(data)\n",
    "joblib.dump(enc, 'enc.pkl')\n",
    "labelled_set = enc.transform(data)\n",
    "labelled_set.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "names = ['MultipleLines', 'InternetService', 'Contract', 'PaymentMethod', 'OnlineSecurity', 'OnlineBackup',\n",
    "         'DeviceProtection', 'TechSupport']\n",
    "\n",
    "ohe = ce.OneHotEncoder(cols=names)\n",
    "ohe.fit(labelled_set)\n",
    "joblib.dump(ohe, 'ohe.pkl')\n",
    "final_set = ohe.transform(labelled_set)\n",
    "final_set.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Now we use scikit-learn's 'train_test_split' function to randomly split our data into training and testing sets. Then remove the _Churn_ and _customerID_ fields from our training and testing datasets and output the shaope of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = final_set['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_set, labels, test_size=0.2)\n",
    "X_train.pop('Churn')\n",
    "X_train.pop('customerID')\n",
    "X_test.pop('Churn')\n",
    "X_test.pop('customerID')\n",
    "print ('Training Data Shape',X_train.shape, y_train.shape)\n",
    "print ('Testing Data Shape',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data For cross validation and GridSearch\n",
    "Y = final_set['Churn']\n",
    "X = final_set.drop(['Churn', 'customerID'], axis=1)\n",
    "print ('Training Data Shape', X.shape)\n",
    "print ('Testing Data Shape', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create DecisionTreeClassifier object, extract hyper parameters, and then GridSearch will best_model from the various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create decision tree object\n",
    "DT = DecisionTreeClassifier()\n",
    "# List of parameters\n",
    "# entropy\n",
    "criterion = ['gini']\n",
    "max_depth = [5,10,15]\n",
    "min_samples_split = [2,4,6]\n",
    "min_samples_leaf = [4,5,6,8]\n",
    "# Save all the lists in the variable\n",
    "hyperparameters = dict(max_depth=max_depth, criterion=criterion,min_samples_leaf = min_samples_leaf ,min_samples_split = min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = GridSearchCV(DT, hyperparameters, cv=5, verbose=0)\n",
    "best_model = model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mean cross validated score\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n",
    "# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\n",
    "print('Best criteria:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best depth:', best_model.best_estimator_.get_params()['max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use K-Folds cross-validator to split data in train/test sets. Create a dictionary of hyperparameter candidates, train model using a DecisionTreeClassifier, assess results, print and store hyper parameters and accuracy and tag using 'DecisionTreeClassifier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 3)\n",
    "hyperparameters = dict(max_depth=5, criterion='gini',min_samples_leaf = 3 ,min_samples_split = 10)\n",
    "model = DecisionTreeClassifier(max_depth=5, criterion='gini',min_samples_leaf = 3 ,min_samples_split = 10)\n",
    "model = model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'dct.pkl')\n",
    "results = model_selection.cross_val_score(model,X,Y,cv = kfold)\n",
    "print(results)\n",
    "print('Accuracy',results.mean()*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Like before, in this next section, on the third line, change experiment_name by appending your username to _customerchurn_, e.g., if your username is user1: \n",
    "#### experiment_name = \"customerchurnuser1\"\n",
    "### Create RandomForestClassifier object, extract hyper parameters, and then the best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%d%Y%H%M%S%f\")\n",
    "experiment_name = \"customerchurnuser29\"\n",
    "experiment_id = experiment_name + timestampStr\n",
    "\n",
    "\n",
    "# Create random forest object\n",
    "RF = RandomForestClassifier()\n",
    "n_estimators = [18,22]\n",
    "criterion = ['gini', 'entropy']\n",
    "# Create a list of all of the parameters\n",
    "max_depth = [30,40,50]\n",
    "min_samples_split = [6,8]\n",
    "min_samples_leaf = [8,10,12]\n",
    "# Merge the list into the variable\n",
    "hyperparameters = dict(n_estimators = n_estimators,max_depth=max_depth, criterion=criterion,min_samples_leaf = min_samples_leaf ,min_samples_split = min_samples_split)\n",
    "# Fit your model using gridsearch\n",
    "model = GridSearchCV(RF, hyperparameters, cv=5, verbose=0)\n",
    "best_model = model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract best scores, params, criteria and depth from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mean cross validated score\n",
    "print('Mean Cross-Validated Score: ',best_model.best_score_)\n",
    "print('Best Parameters',best_model.best_params_)\n",
    "# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\n",
    "print('Best criteria:', best_model.best_estimator_.get_params()['criterion'])\n",
    "print('Best depth:', best_model.best_estimator_.get_params()['max_depth'])\n",
    "print('Best estimator:', best_model.best_estimator_.get_params()['n_estimators'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### As above, use K-Folds cross-validator to split data in train/test sets. Create a dictionary of hyperparameter candidates, train model using a RandomForestClassifier, assess results, print and store hyper parameters and accuracy and tag using 'RandomForestClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 3)\n",
    "hyperparameters = dict(max_depth=40, criterion='gini',min_samples_leaf = 12 ,min_samples_split = 8, n_estimators = 22)\n",
    "model = RandomForestClassifier(max_depth=40, criterion='gini',min_samples_leaf = 12 ,min_samples_split = 8, n_estimators = 22)\n",
    "model = model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'rft.pkl')\n",
    "results = model_selection.cross_val_score(model,X,Y,cv = kfold)\n",
    "print(results)\n",
    "print('Accuracy',results.mean()*100)\n",
    "# store = get_meta_store()\n",
    "# store.log_hyperparameters(hyperparameters)\n",
    "# store.log_model(model)\n",
    "# store.log_metric('Accuracy',results.mean()*100)\n",
    "# store.log_tag(\"RandomForestClassifier\")\n",
    "# store.log_attribute(\"data_file_location\", \"data/full_data_csv/a.csv\")\n",
    "# store.log_attribute(\"data_file_version\", data_file_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Notebook complete')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
